\documentclass[11pt]{article}

% ── packages ──────────────────────────────────────────────────────────────
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}

\graphicspath{{../pca_sweep_plots/}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\defect}{\mathcal{D}}

\title{Emergent Low-Dimensional Invariant Submanifolds in Grokking Dynamics}

\author{%
  Yongzhong Xu
  \thanks{abbyxu@gmail.com ; code at https://github.com/skydancerosel/grokking-integrability}
}


\date{}

\begin{document}
\maketitle

% ══════════════════════════════════════════════════════════════════════════
\begin{abstract}
% ══════════════════════════════════════════════════════════════════════════

Grokking---the abrupt onset of generalization long after memorization---challenges standard accounts of how neural networks learn, yet the geometric mechanisms driving this transition remain poorly understood.
We identify an emergent low-dimensional invariant submanifold---the \emph{execution manifold}---in the weight space of transformers trained on modular arithmetic.
Using PCA on attention weight trajectories and commutator defect analysis across six binary operations mod~97, we show that weight evolution during grokking is essentially one-dimensional: a single principal component captures 70--94\% of variance across 36 experimental conditions.
We then measure loss-landscape curvature via commutator defects---the non-commutativity of successive gradient steps---and project these onto the learned submanifold.
The commutator vectors are predominantly orthogonal to the execution manifold, even relative to random baselines (exec/random ratio $\approx 2$--$3\times$), indicating that the submanifold is \emph{invariant} under the optimization dynamics: curvature does not deflect the trajectory out of its learned subspace.
Yet curvature explodes orthogonally: grokking operations show $10$--$1000\times$ higher commutator defect than non-grokking controls, concentrated entirely in the normal bundle of the execution manifold.
The onset of this curvature growth consistently \emph{precedes} the generalization transition by 600--1600 training steps (sign test $p = 2^{-12} < 0.001$), though non-grokking operations also exhibit moderate curvature growth ($30$--$50\times$) without generalizing, so onset is necessary but not sufficient.
All findings replicate across a $100\times$ learning rate sweep, a qualitatively different slow regime ($\mathrm{lr}=5\!\times\!10^{-5}$, $\mathrm{wd}=0.1$, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes.
Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose--response across four operations, while artificially boosting curvature defects has no effect.

\end{abstract}

% ══════════════════════════════════════════════════════════════════════════
\section{Introduction}
\label{sec:intro}
% ══════════════════════════════════════════════════════════════════════════

Grokking---the phenomenon where neural networks trained on small algorithmic datasets first memorize the training set and then, long after achieving perfect training accuracy, suddenly generalize to the test set---was first reported by \citet{power2022grokking} in modular arithmetic tasks.
The phenomenon has attracted significant attention because it challenges the conventional understanding that generalization and memorization are tightly coupled in optimization dynamics.

Prior work has characterized grokking through the lens of representation learning \citep{nanda2023grokking}, weight decay as implicit regularization \citep{liu2022omnigrok}, circuit formation \citep{zhong2024clock}, and phase transitions in loss landscapes.
Recently, \citet{xu2026lowdim} showed that the weight-space trajectory during grokking lies on a low-dimensional execution manifold, with PCA revealing that a single principal component captures the majority of trajectory variance.
However, a fundamental geometric question remains open: \emph{is this low-dimensional manifold invariant under the optimization dynamics---does curvature deflect the trajectory out of its learned subspace---and does its geometry predict when generalization will occur?}

Building on \citet{xu2026lowdim}, we address this question by studying the differential geometry of the parameter-space trajectory during grokking.
Our approach extends the PCA eigenanalysis with a new tool---commutator defect analysis---that probes the curvature structure of the loss landscape relative to the learned submanifold:
\begin{enumerate}
    \item \textbf{PCA eigenanalysis} of attention weight trajectories, revealing the intrinsic dimensionality of the learned submanifold;
    \item \textbf{Commutator defect analysis}, measuring loss-landscape curvature and its relationship to the learned submanifold.
\end{enumerate}

The commutator defect quantifies the non-commutativity of successive gradient steps: given two mini-batches $A$ and $B$, the defect measures how much the final parameter vector depends on the order of gradient updates.
In a flat region of the loss landscape, gradient steps commute; in a curved region, they do not.
By projecting these commutator vectors onto the PCA submanifold, we can determine whether the learned subspace is flat or curved.

\paragraph{Key contributions.}
Our work makes five main contributions, spanning observation, prediction, and causal testing:
\begin{enumerate}
    \item \textbf{Rank-1 manifold}: The weight-space trajectory during grokking lies on a rank-1 submanifold (70--94\% of variance in PC1).
    \item \textbf{Invariant submanifold}: This submanifold is invariant under the optimization dynamics: commutator defects are predominantly orthogonal to it ($\rho \approx 1.000$ within numerical precision across 36 conditions, with exec/random projection ratio $\approx 2$--$3\times$), meaning curvature does not deflect the trajectory out of its learned subspace.
    \item \textbf{Temporal ordering}: Curvature explodes orthogonally during grokking ($10$--$1000\times$ increase) and its onset consistently \emph{precedes} generalization by 600--1600 steps. Non-grokking operations show moderate curvature growth ($30$--$50\times$) without generalization, so onset is necessary but not sufficient.
    \item \textbf{Causal interventions}: Suppressing orthogonal gradient flow prevents grokking (necessary) while boosting curvature defects has no effect (not sufficient), establishing an asymmetric causal relationship.
    \item \textbf{Robustness}: All results replicate across a $100\times$ learning rate sweep, a $200\times$ timescale difference between regimes, four operations, and three seeds.
\end{enumerate}

\paragraph{Paper outline.}
We proceed as follows.
\Cref{sec:setup} describes the experimental setup.
\Cref{sec:methods} introduces our geometric tools: PCA eigenanalysis, commutator defect, manifold projection, and trajectory--curvature alignment.
\Cref{sec:results} presents results in three stages: geometric structure (\Cref{sec:rank1}--\Cref{sec:curvature}), predictive power and robustness (\Cref{sec:prediction}--\Cref{sec:lr_sweep}), and causal interventions (\Cref{sec:interventions}).
\Cref{sec:discussion} discusses implications and connections to broader themes.


% ══════════════════════════════════════════════════════════════════════════
\section{Experimental Setup}
\label{sec:setup}
% ══════════════════════════════════════════════════════════════════════════

\subsection{Model and Training}

We use a Transformer encoder following the canonical grokking setup of \citet{power2022grokking}.
The model processes two integer tokens $a, b \in \{0, \ldots, p-1\}$ (with $p = 97$) and predicts $f(a,b) \bmod p$ for a binary operation $f$.

\paragraph{Architecture.}
The model consists of:
\begin{itemize}
    \item A token embedding $\mathrm{Emb}: \{0,\ldots,96\} \to \R^{128}$ plus a learnable positional embedding $\mathbf{P} \in \R^{2 \times 128}$;
    \item A 2-layer Transformer encoder with pre-norm (LayerNorm before attention and FFN), $d_\text{model} = 128$, 4 attention heads, $d_\text{ff} = 256$, GELU activation, no dropout;
    \item A final LayerNorm followed by a linear head $\R^{128} \to \R^{97}$ applied to the first token position.
\end{itemize}
The total parameter count is approximately 290k.

\paragraph{Training.}
We train with AdamW ($\beta_1 = 0.9$, $\beta_2 = 0.98$) at learning rate $10^{-3}$ with weight decay $\lambda = 1.0$ (or $\lambda = 0.0$ for non-grokking controls), batch size 512, gradient clipping at 1.0, and a 50/50 train/test split.
Training runs for up to 200k steps with early stopping when test accuracy exceeds 98\% for 3 consecutive evaluations.

\paragraph{Operations.}
We test six binary operations mod~97, four of which exhibit grokking under these hyperparameters and two that do not (\Cref{tab:operations}).

\begin{table}[ht]
\centering
\caption{Operations tested. Grok step is the mean step at which test accuracy reaches 90\%, averaged over 3 seeds.}
\label{tab:operations}
\begin{tabular}{@{}llcc@{}}
\toprule
Operation & Formula & Groks? & Grok step \\
\midrule
add & $(a+b) \bmod 97$ & Yes & $\sim$2900 \\
sub & $(a-b) \bmod 97$ & Yes & $\sim$3400 \\
mul & $(a \times b) \bmod 97$ & Yes & $\sim$2600 \\
x2\_y2 & $(a^2+b^2) \bmod 97$ & Yes & $\sim$1900 \\
x2\_xy\_y2 & $(a^2+ab+b^2) \bmod 97$ & No & --- \\
x3\_xy & $(a^3+ab) \bmod 97$ & No & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyperparameter Regimes}

To test regime invariance, we additionally run a \textbf{slow regime} with qualitatively different hyperparameters: $\mathrm{lr} = 5 \times 10^{-5}$, $\lambda = 0.1$, 3 Transformer layers, and $\beta_2 = 0.999$.
In this regime, grokking occurs at $\sim$570k steps (vs.\ $\sim$3k in the fast regime), providing a $200\times$ difference in training timescale.

\subsection{Attention Weight Logging}

During training, we log the four attention weight matrices---$W_Q$, $W_K$, $W_V$ (extracted from the fused \texttt{in\_proj\_weight}) and $W_O$ (\texttt{out\_proj.weight})---every 100 steps.
Each matrix is $128 \times 128$ (or $128 \times 32$ per head), giving a trajectory of snapshots for subsequent PCA analysis.


% ══════════════════════════════════════════════════════════════════════════
\section{Methods}
\label{sec:methods}
% ══════════════════════════════════════════════════════════════════════════

\subsection{PCA Eigenanalysis of Weight Trajectories}
\label{sec:pca}

For each attention weight matrix $W \in \R^{d \times d}$, we collect $T$ training snapshots $\{W_t\}_{t=1}^T$ and compute PCA on the flattened trajectory of weight \emph{changes} from initialization:
\begin{equation}
    X = \begin{bmatrix} \mathrm{vec}(W_1 - W_0) \\ \vdots \\ \mathrm{vec}(W_T - W_0) \end{bmatrix} \in \R^{T \times d^2},
\end{equation}
after centering columns.
We compute the SVD $X = U \Sigma V^\top$ and define the explained variance ratio of the $k$-th principal component as $\sigma_k^2 / \sum_i \sigma_i^2$.
The quantity PC1\% $= 100 \times \sigma_1^2 / \sum_i \sigma_i^2$ measures the fraction of trajectory variance captured by a single direction.

\subsection{Commutator Defect}
\label{sec:commutator}

\emph{Intuition.}
If the loss landscape is locally flat, the order in which we apply two gradient updates does not matter: updating with batch $A$ then $B$ gives the same result as $B$ then $A$.
In a curved region, the order matters---just as walking east then north on a sphere leads to a different point than north then east.
The commutator defect quantifies this order-dependence, providing a local probe of loss-landscape curvature that requires no Hessian computation.

\emph{Formal construction.}
The commutator defect measures loss-landscape curvature by quantifying the non-commutativity of gradient steps from two independent mini-batches.
Given the current parameters $\theta_0$ and two mini-batches $A, B$:
\begin{align}
    \theta_{AB} &= \theta_0 - \eta\, g_A(\theta_0) - \eta\, g_B(\theta_0 - \eta\, g_A(\theta_0)) \\
    \theta_{BA} &= \theta_0 - \eta\, g_B(\theta_0) - \eta\, g_A(\theta_0 - \eta\, g_B(\theta_0))
\end{align}
where $g_A(\theta) = \nabla_\theta \mathcal{L}_A(\theta)$ is the gradient of the cross-entropy loss on mini-batch $A$ at parameters $\theta$, and $\eta = 10^{-3}$ is a fixed step size.
The (scale-normalized) commutator defect is:
\begin{equation}
\label{eq:defect}
    \defect = \frac{\norm{\theta_{AB} - \theta_{BA}}}{\norm{\eta\, g_A} \cdot \norm{\eta\, g_B}}.
\end{equation}
We justify this via first-order Taylor expansion: to leading order in $\eta$, $\theta_{AB} - \theta_{BA} \approx \eta^2 (\nabla g_B \cdot g_A - \nabla g_A \cdot g_B)$, which is the Lie bracket of the gradient vector fields.
Geometrically, $\defect$ thus measures the Riemann curvature of the loss landscape in the directions $g_A, g_B$: if the landscape is locally flat, gradient steps commute and $\defect = 0$.

We compute $K = 9$ independent samples of $\defect$ at each measurement point and report the median, providing a robust estimate.

\subsection{Projection onto the PCA Manifold}
\label{sec:projection}

To determine whether loss-landscape curvature lives inside or outside the learned submanifold, we construct an orthonormal basis $B \in \R^{P \times K}$ for the PCA subspace embedded in full parameter space ($P \approx 290$k).

For each Transformer layer and each attention weight matrix $\{W_Q, W_K, W_V, W_O\}$:
\begin{enumerate}
    \item Compute the top-2 PCA directions from the weight trajectory (each a vector in $\R^{d^2}$);
    \item Embed each direction into the full parameter space at the correct offset;
    \item Stack all embedded directions and orthonormalize via QR decomposition.
\end{enumerate}

Given a commutator vector $\delta = \theta_{AB} - \theta_{BA}$, we decompose it as:
\begin{equation}
    \delta = \underbrace{B\, B^\top \delta}_{\delta_\parallel\;\text{(projected)}} + \underbrace{(\delta - B\, B^\top \delta)}_{\delta_\perp\;\text{(residual)}}.
\end{equation}
The \textbf{invariance measure} is the residual fraction:
\begin{equation}
\label{eq:invariance}
    \rho = \frac{\norm{\delta_\perp}}{\norm{\delta}}.
\end{equation}
If $\rho \approx 1$, the commutator is orthogonal to the PCA subspace, indicating that loss-landscape curvature is confined to the normal bundle and the execution manifold is invariant under the optimization dynamics---curvature does not deflect the trajectory out of its learned subspace.
If $\rho \approx 0$, curvature lies within the learned subspace and the submanifold is not invariant.

\subsection{Random Subspace Control}
\label{sec:random_control}

To verify that the near-zero projection fraction $1 - \rho$ reflects genuine geometric structure rather than a trivial dimensionality artifact---any $K$-dimensional subspace of $\R^P$ captures $\sim\!\sqrt{K/P}$ of a random vector---we compare the PCA-basis projection against a random baseline.
For each commutator vector $\delta$, we compute the projection fraction onto $N_\text{rand} = 5$ random $K$-dimensional orthonormal bases (generated via QR decomposition of Gaussian random matrices) and average the results.
The ratio $\text{proj}_{\text{exec}} / \text{proj}_{\text{rand}}$ is the key diagnostic: values significantly above $1.0$ confirm that the PCA subspace captures more commutator energy than expected by chance.
Because absolute projection magnitudes vanish in high-dimensional spaces ($\text{proj}/\text{full} \sim \sqrt{K/P} \ll 1$ for any $K$-dimensional subspace of $\R^P$), only normalized comparisons to random subspaces are geometrically meaningful; accordingly, we report $\rho$ alongside the exec/random ratio throughout.

\subsection{Converse Analysis: Trajectory Alignment with Curvature}
\label{sec:converse}

As a converse test, we ask whether the weight trajectory \emph{avoids} high-curvature directions.
At each checkpoint, we compute the mean absolute cosine similarity between the trajectory step $\Delta\theta_t = \theta_t - \theta_{t-1}$ and $K = 12$ commutator vectors $\{\delta_k\}$:
\begin{equation}
    \bar{c}_t = \frac{1}{K} \sum_{k=1}^K \frac{|\Delta\theta_t \cdot \delta_k|}{\norm{\Delta\theta_t}\,\norm{\delta_k}}.
\end{equation}
For comparison, the expected absolute cosine between random vectors in $\R^P$ is $\sqrt{2/(\pi P)} \approx 1.5 \times 10^{-3}$ for $P = 290$k.
If $\bar{c}_t$ is near this baseline, the trajectory is not aligned with curvature directions.


% ══════════════════════════════════════════════════════════════════════════
\section{Results}
\label{sec:results}
% ══════════════════════════════════════════════════════════════════════════

We present our findings in three stages.
First, we establish the geometric structure of grokking: rank-1 weight trajectories on an invariant submanifold with orthogonal curvature explosion (\Cref{sec:rank1}--\Cref{sec:curvature}).
Second, we characterize the temporal relationship between curvature growth and generalization, and demonstrate robustness across regimes and learning rates (\Cref{sec:prediction}--\Cref{sec:lr_sweep}).
Third, we test causality through targeted interventions (\Cref{sec:interventions}).

\subsection{Weight Evolution is Rank-1}
\label{sec:rank1}

PCA on attention weight trajectories reveals that the first principal component captures 70--94\% of trajectory variance across all grokking conditions (\Cref{fig:pca_overview}).
Weight evolution during grokking is essentially one-dimensional.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figA_grok_vs_nowd_crossop.png}
        \caption{PC1\% for grokking (wd=1.0) vs.\ no-wd (wd=0.0) across operations. Grokking operations consistently show high PC1\%.}
        \label{fig:pca_grok_vs_nowd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figC_eigenspectrum_crossop.png}
        \caption{Top-5 eigenspectrum per operation. The first eigenvalue dominates across all operations.}
        \label{fig:eigenspectrum}
    \end{subfigure}
    \caption{Weight trajectories during grokking are rank-1. \textbf{(a)} PC1\% across operations: grokking runs (wd=1.0) show 70--94\% variance in a single component. \textbf{(b)} Eigenspectrum showing dominant first eigenvalue.}
    \label{fig:pca_overview}
\end{figure}

No-weight-decay controls ($\lambda = 0$) also show moderately high PC1\%, but the null model comparison reveals that grokking PC1\% values are 5--20 standard deviations above the random-walk baseline (\Cref{fig:null_model}), confirming the concentration is not an artifact of trajectory smoothness.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figE_null_zscores_crossop.png}
        \caption{Z-scores vs.\ random-walk null model. All operations exceed the null by $>5\sigma$.}
        \label{fig:null_model}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figF_temporal_crossop.png}
        \caption{Temporal evolution of PC1\% during training. Concentration increases as grokking progresses.}
        \label{fig:temporal_pca}
    \end{subfigure}
    \caption{PCA concentration is genuine and increases over training. \textbf{(a)} Z-scores above random-walk null. \textbf{(b)} Expanding-window PC1\% over training.}
    \label{fig:pca_controls}
\end{figure}


\subsection{The Execution Manifold is Invariant Under the Dynamics}
\label{sec:integrability}

Having established that the weight trajectory lies on a low-dimensional submanifold, we ask: is this submanifold invariant under the optimization dynamics---does loss-landscape curvature deflect the trajectory out of its learned subspace, or is curvature confined to orthogonal directions?

We compute commutator defects at regular checkpoints during training and project each commutator vector onto the PCA basis (\Cref{sec:projection}).
The key result: the residual fraction $\rho = \norm{\delta_\perp}/\norm{\delta}$ is $\approx 1.000$ within numerical precision across all 36 conditions (6 operations $\times$ 2 weight-decay settings $\times$ 3 seeds), as shown in \Cref{fig:integrability}.
Curvature is confined entirely to the normal bundle of the execution manifold; the submanifold is invariant under the optimization flow.
We note that this near-unity value reflects the high dimensionality of parameter space ($P \approx 290$k) relative to the PCA subspace ($K = 16$--$24$); the exec/random ratio (\Cref{sec:random_control}) provides the complementary test that the small parallel component is geometrically meaningful.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figK_integrability.png}
        \caption{Invariance measure: the residual fraction $\rho \approx 1.0$ at every checkpoint, meaning commutator vectors are predominantly orthogonal to the execution manifold.}
        \label{fig:integrability_single}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figS_multiseed_integrability.png}
        \caption{Multi-seed replication: $\rho \approx 1.000$ within numerical precision across all 36 conditions, confirming invariance.}
        \label{fig:integrability_multiseed}
    \end{subfigure}
    \caption{The execution manifold is invariant under the optimization dynamics. Commutator defect vectors are predominantly orthogonal to the PCA subspace, with curvature confined to the normal bundle.}
    \label{fig:integrability}
\end{figure}

This means that the dominant component of loss-landscape curvature lies \emph{outside} the directions the model actually uses for learning.
The weight trajectory evolves on an invariant submanifold: despite enormous curvature in the ambient parameter space, the curvature is confined to orthogonal directions and does not deflect the optimization trajectory out of its learned subspace.

\paragraph{Random subspace control.}
To confirm that the near-zero projection onto the PCA basis reflects genuine geometry rather than a dimensionality artifact, we compare against random $K$-dimensional subspaces (\Cref{sec:random_control}).
\Cref{fig:random_control} shows the projection fraction for the PCA (execution) basis and the random baseline over training.
Across all four grokking operations, the execution basis captures $1.8$--$2.9\times$ more commutator energy than a random subspace of equal dimension ($K = 24$), confirming that the small parallel component is geometrically structured.
We verify that this ratio is stable under variation of the PCA dimension $K$: reducing to $K = 16$ or increasing to $K = 32$ yields exec/random ratios within the same range, confirming that the result is not an artifact of the particular choice of $K$.
Crucially, both projections are very small (proj/full $< 0.05$), consistent with the invariance measure $\rho \approx 1.000$: in a space of $\sim$290k dimensions, any $K$-dimensional subspace captures negligible energy from a generic vector.
The PCA subspace nonetheless captures a structured excess above the random floor, confirming that the near-unity $\rho$ reflects genuine geometric orthogonality rather than merely high dimensionality.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figC1_exec_vs_random.png}
        \caption{Projection fraction (proj/full) for execution basis (green) vs.\ random baseline (red) over training. The execution basis consistently captures more commutator energy.}
        \label{fig:exec_vs_random}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figC5_hero.png}
        \caption{Combined view: commutator defect (red), exec/random ratio (green), and test accuracy (blue, dashed). The exec/random ratio is consistently above 1.0 during grokking.}
        \label{fig:hero_exec_random}
    \end{subfigure}
    \caption{Random subspace control confirms that the PCA projection is geometrically structured, not a dimensionality artifact. Exec/random ratio $\approx 1.8$--$2.9\times$ across operations.}
    \label{fig:random_control}
\end{figure}

\subsection{Curvature Explodes Orthogonally During Grokking}
\label{sec:curvature}

While the execution manifold remains invariant (curvature confined to the normal bundle), the \emph{magnitude} of curvature in orthogonal directions changes substantially during grokking.
Operations that grok show 10--1000$\times$ higher commutator defect than non-grokking controls (\Cref{fig:defect_comparison}), and this curvature is concentrated predominantly outside the PCA manifold.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figL_grok_vs_nowd_commutator.png}
        \caption{Grok (wd=1.0) vs.\ no-wd (wd=0.0): grokking runs develop substantially higher commutator defect.}
        \label{fig:grok_vs_nowd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figU_multiseed_defect.png}
        \caption{Commutator defect across all conditions (3 seeds). Grokking operations (wd=1.0) show 10--1000$\times$ higher defect.}
        \label{fig:defect_multiseed}
    \end{subfigure}
    \caption{Curvature explodes during grokking but remains orthogonal to the learned subspace.}
    \label{fig:defect_comparison}
\end{figure}

The converse analysis confirms that the weight trajectory does not align with curvature directions: the mean absolute cosine similarity between trajectory steps and commutator vectors is indistinguishable from the random-vector baseline ($\bar{c} \approx \sqrt{2/(\pi P)}$), meaning the trajectory actively avoids high-curvature directions.


\subsection{Curvature Growth Precedes Generalization}
\label{sec:prediction}

A key finding is that the onset of commutator defect growth consistently \emph{precedes} the generalization transition.
We define the \textbf{defect onset} as the first training step at which the commutator defect exceeds $10\times$ its early-training baseline (median of the first 3 measurements) and an absolute threshold of~20.
\Cref{fig:prediction} shows the temporal overlay of commutator defect and test accuracy for all four grokking operations and two non-grokking controls.
In every grokking run, defect begins rising before test accuracy increases.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figW_defect_predicts_grokking.png}
    \caption{Temporal ordering of curvature growth and generalization. Top four panels: grokking operations (3 seeds each), showing defect (solid) rising before test accuracy (dashed). Bottom two panels: non-grokking controls show moderate defect growth ($30$--$50\times$ baseline) but no generalization. Dotted vertical lines mark defect onset; green regions mark grokking.}
    \label{fig:prediction}
\end{figure}

Across all 12 grokking runs (4 operations $\times$ 3 seeds), defect onset precedes the point at which test accuracy reaches 90\% by 600--1600 steps, with mean lead time of 1117 steps (\Cref{fig:lead_time}).
A one-sided sign test gives $p = 2^{-12} \approx 2.4 \times 10^{-4}$, confirming that the temporal ordering is statistically significant.

\paragraph{Necessary but not sufficient.}
Non-grokking operations ($a^2 + ab + b^2$ and $a^3 + ab$) also exhibit defect growth---reaching $30$--$50\times$ their early-training baseline---but never generalize.
Grokking operations, by contrast, reach $500$--$2000\times$ baseline, with zero overlap between the two groups in total growth magnitude (minimum grokking: $513\times$; maximum non-grokking: $47\times$).
However, this separation is only apparent retrospectively: at early training steps when grokking has not yet occurred, the defect trajectories of grokking and non-grokking operations overlap.
Defect onset is therefore best understood as a \emph{necessary precondition} for grokking---a signal that the loss landscape is reorganizing---rather than a sufficient predictor that discriminates which operations will generalize.
The causal intervention experiments (\Cref{sec:interventions}) confirm this interpretation: suppressing orthogonal gradient flow prevents grokking, establishing the mechanistic necessity of curvature growth.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figW2_hero_defect_predicts_grok.png}
        \caption{Hero example: $(a-b) \bmod 97$, seed 137. Defect onset at step 2000, grokking at step 3600 (lead = 1600 steps).}
        \label{fig:hero_defect_predicts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figX_defect_lead_time.png}
        \caption{Lead time quantification. Left: onset step vs.\ grok step (all points above diagonal). Right: lead time by operation (sign test $p < 0.001$).}
        \label{fig:lead_time}
    \end{subfigure}
    \caption{Temporal ordering: curvature growth precedes generalization.}
    \label{fig:prediction_detail}
\end{figure}


\subsection{Regime Invariance}
\label{sec:regime}

To verify that our findings are not specific to a particular hyperparameter setting, we repeat the full analysis in a slow regime with qualitatively different hyperparameters (\Cref{tab:regimes}).

\begin{table}[ht]
\centering
\caption{Hyperparameter regimes and key metrics.}
\label{tab:regimes}
\begin{tabular}{@{}lcc@{}}
\toprule
Metric & Fast regime & Slow regime \\
\midrule
Learning rate & $10^{-3}$ & $5 \times 10^{-5}$ \\
Weight decay & 1.0 & 0.1 \\
Layers & 2 & 3 \\
Adam $\beta_2$ & 0.98 & 0.999 \\
\midrule
Grok step (add, mean) & $\sim$2,900 & $\sim$570,000 \\
Invariance ($\rho$) & $\approx 1.000$ & $\approx 1.000$ \\
Onset precedes grok? & 12/12 runs & 2/2 runs \\
Lead time (absolute) & $\sim$1,100 steps & $\sim$558,000 steps \\
Lead time (normalized) & $\sim$0.38 & $\sim$0.55 \\
\bottomrule
\end{tabular}
\end{table}

\Cref{fig:slow_regime} shows the slow-regime results.
Despite a $200\times$ difference in grokking timescale, $10\times$ difference in weight decay, and a different number of layers, the same qualitative transition is observed: the execution manifold is invariant ($\rho \approx 1.000$), and defect onset precedes grokking by hundreds of thousands of steps.
However, the alignment dynamics differ quantitatively between regimes.
In the fast regime, trajectory--curvature alignment is initially above the random baseline and decays toward the transition, consistent with underdamped exploration of parameter space.
In the slow regime, alignment remains at or below the random baseline throughout, consistent with overdamped motion along narrow valleys.
Both regimes nonetheless exhibit a defect-mediated generalization transition.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figY_regime_comparison_commutator.png}
        \caption{Regime comparison: invariance, defect, and normalized lead time are consistent across regimes.}
        \label{fig:regime_comparison}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figZ_slow_regime_hero.png}
        \caption{Slow regime hero: defect onset at $\sim$185k, grokking at $\sim$585k (lead $\approx$ 400k steps).}
        \label{fig:slow_hero}
    \end{subfigure}
    \caption{Regime invariance: qualitative findings replicate in the slow regime ($200\times$ longer training), though alignment dynamics differ quantitatively.}
    \label{fig:slow_regime}
\end{figure}


\subsection{Learning Rate Phase Diagram}
\label{sec:lr_sweep}

Having established regime invariance between qualitatively different configurations, we now systematically vary the learning rate alone to map the phase boundary of grokking dynamics.
We sweep $\eta \in \{10^{-4},\, 10^{-3},\, 10^{-2}\}$ with fixed $\lambda = 1.0$ across all six operations and three seeds (54 runs total).

\Cref{fig:lr_phase} shows the resulting phase diagram.
The grok/no-grok boundary is \emph{invariant} to learning rate: the same four operations grok at all three rates, while the two complex operations never grok (\Cref{fig:lr_phase}A).
Grokking speed scales roughly linearly with $\eta$: mean grok steps are $\sim$34k at $\eta = 10^{-4}$, $\sim$3k at $\eta = 10^{-3}$, and $\sim$500--4000 at $\eta = 10^{-2}$ (\Cref{fig:lr_phase}B).

The defect landscape reveals a striking asymmetry (\Cref{fig:lr_phase}C): at low learning rate, maximum defect reaches $10^4$, while at high learning rate it drops to $\sim$20--60.
This suggests that slower optimization allows curvature to accumulate more in the orthogonal bundle before the phase transition occurs.

The predictive lead time (\Cref{fig:lr_phase}D) is largest at $\eta = 10^{-4}$ ($\sim$23--33k steps) and moderate at $\eta = 10^{-3}$ ($\sim$700--1400 steps).
At $\eta = 10^{-2}$, grokking occurs so rapidly ($<$1k steps) that the memorization and generalization phases overlap, and defect onset is concurrent with rather than predictive of grokking.
\Cref{fig:lr_hero} illustrates these three regimes for the addition operation.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figPD_lr_phase_diagram.png}
    \caption{Phase diagram of grokking dynamics across learning rates ($\eta \in \{10^{-4}, 10^{-3}, 10^{-2}\}$, $\lambda = 1.0$, 3 seeds per cell).
    (A)~Grok fraction: the phase boundary between grokking and non-grokking operations is invariant to learning rate.
    (B)~Mean grok step (log scale): grokking speed scales with $\eta$.
    (C)~Mean max defect (log scale): curvature explosion is largest at low $\eta$.
    (D)~Mean lead time (onset step $-$ grok step): defect onset is most predictive at low $\eta$.}
    \label{fig:lr_phase}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figPD2_lr_sweep_hero.png}
    \caption{Defect and test accuracy trajectories for addition across three learning rates. At $\eta = 10^{-4}$ (blue), defect onset precedes grokking by $\sim$33k steps; at $\eta = 10^{-3}$ (orange), by $\sim$1.4k steps; at $\eta = 10^{-2}$ (green), grokking is nearly instantaneous.}
    \label{fig:lr_hero}
\end{figure}

\paragraph{LR-dependent alignment dynamics.}
To directly test whether the damping regime varies with learning rate, we measure trajectory--curvature alignment (mean $|\cos(\Delta\theta, \delta)|$, \Cref{sec:converse}) at four strategic checkpoints---memorization, defect onset, and post-grok---for each of the three learning rates on two operations (\Cref{fig:lr_alignment}).
At $\eta = 10^{-4}$, alignment remains below the random baseline ($0.18$--$0.86\times$) throughout, consistent with overdamped dynamics where the trajectory is confined to narrow valleys far from curvature directions.
At $\eta = 10^{-2}$, alignment is consistently \emph{above} the baseline ($1.4$--$1.8\times$), indicating underdamped exploration that initially samples curvature directions.
At $\eta = 10^{-3}$, the intermediate regime, alignment starts below baseline and rises toward or above it at the grokking transition.
This LR-dependent pattern replicates across both operations and provides direct evidence for the dynamical regimes discussed below.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figPD3_lr_alignment.png}
    \caption{Trajectory--curvature alignment at three training phases across learning rates.
    At $\eta = 10^{-4}$ (blue), alignment stays below the random baseline (dotted), consistent with overdamped dynamics.
    At $\eta = 10^{-2}$ (green), alignment exceeds the baseline, consistent with underdamped exploration.
    $\eta = 10^{-3}$ (orange) shows intermediate behavior.
    Both operations exhibit the same pattern.}
    \label{fig:lr_alignment}
\end{figure}

To integrate curvature accumulation and trajectory geometry into a unified picture, we construct a reduced phase portrait using the commutator defect and the trajectory--curvature alignment as coordinates (\Cref{fig:alignment_vs_defect}).
Each training run traces a characteristic path through this space, progressing from memorization through defect onset to the post-grokking regime.

We observe three qualitatively distinct dynamical regimes controlled by the learning rate.
At high learning rates, training remains in an underdamped regime, exhibiting strong alignment with curvature directions and low defect accumulation.
At low learning rates, training becomes overdamped, with prolonged confinement to low-alignment regions and substantial defect buildup prior to grokking.
Intermediate learning rates interpolate between these behaviors, producing critically damped trajectories.

Across both addition and multiplication tasks, grokking occurs when trajectories exit a metastable region characterized by high curvature defect and suppressed mobility.
This phase portrait provides a compact geometric representation of the grokking transition and clarifies how optimization hyperparameters control the pathway to algorithmic generalization.
To our knowledge, this is the first identification of distinct overdamped, critically damped, and underdamped dynamical regimes in grokking, suggesting that the phenomenon possesses a richer phase structure than previously recognized.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figPD4_alignment_vs_defect.png}
    \caption{Phase portrait of grokking dynamics in curvature--trajectory space.
    We plot the trajectory--curvature alignment ratio (mean\,$|\!\cos(\Delta\theta,\delta)\!|$ normalized by random baseline) against the commutator defect magnitude for three learning rates ($\eta = 10^{-4}, 10^{-3}, 10^{-2}$).
    Each polyline traces the evolution from memorization (\textsc{mem}) through defect onset (\textsc{spike}) to the post-grokking regime (\textsc{post}), shown for addition (circles) and multiplication (squares); arrows indicate the direction of training.
    The horizontal dashed line indicates isotropic alignment.
    Training at high learning rates remains in an underdamped regime characterized by strong alignment and low defect, while low learning rates produce overdamped dynamics with large defect accumulation and weak alignment.
    Intermediate learning rates interpolate between these regimes.
    Stars mark checkpoints where test accuracy exceeds $90\%$.
    Grokking corresponds to escape from a metastable region of high curvature defect and reduced mobility, with regime-dependent relaxation dynamics.}
    \label{fig:alignment_vs_defect}
\end{figure}


% ══════════════════════════════════════════════════════════════════════════
\subsection{Causal Interventions on Learning Dynamics}
\label{sec:interventions}
% ══════════════════════════════════════════════════════════════════════════

To test whether the observed defect accumulation and geometric reorganization are merely correlational or causally involved in grokking, we perform a series of targeted intervention experiments.
These experiments modify the optimization trajectory while preserving the underlying architecture and dataset, allowing us to probe necessity and sufficiency.

\subsubsection{Gradient Subspace Suppression}
\label{sec:suppression}

We first examine whether motion along the learned execution manifold is necessary for grokking.
At each training step after step 500 (post-memorization), we project the gradient onto the subspace spanned by the top principal components of the weight trajectory, with projection strength $s \in [0, 1]$:
\begin{equation}
    g \;\longrightarrow\; g_\parallel + (1 - s)\, g_\perp, \qquad g_\parallel = B\, B^\top g,\quad g_\perp = g - g_\parallel,
\end{equation}
where $B \in \R^{P \times K}$ is the PCA basis from Phase~1 training.
For comparison, we also apply random low-dimensional projections of equal rank ($K = 16$).

Partial suppression along the PCA directions ($s = 0.25$--$0.75$) systematically delays grokking, while full projection ($s = 1.0$) completely prevents generalization (0/12 seeds across four operations; \Cref{fig:multiop_dose}).
In contrast, random projections have little effect at intermediate strengths ($<$\,50-step difference from baseline; \Cref{fig:ablation_pca_random}).
At $s = 1.0$, both projections kill grokking, since confining the optimizer to \emph{any} 16-dimensional subspace of $\R^{290\text{k}}$ is too restrictive.
These results indicate that grokking requires access to specific learned directions in parameter space, rather than arbitrary low-dimensional motion.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figI10_multiop_dose_response.png}
    \caption{Dose--response curve for gradient projection across all four grokking operations.
    Each panel shows mean grok step (3 seeds) vs.\ suppression strength $s$.
    At $s = 1.0$, grokking fails universally (0/12 seeds).
    Dashed line: baseline (no intervention).
    The monotonic delay and complete suppression at full strength replicate across all operations.}
    \label{fig:multiop_dose}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figI6_ablation_random_vs_pca.png}
    \caption{PCA-specific suppression control.
    Left: grok step vs.\ suppression strength for PCA projection (blue) and random projection (green); dashed line is baseline.
    Right: grok success rate.
    At intermediate strengths ($s = 0.25$--$0.75$), PCA projection monotonically delays grokking while random projection has no effect.
    At $s = 1.0$, both kill grokking (any 16-dim constraint is too restrictive).
    The dose--response separation confirms the geometric specificity of the PCA manifold.}
    \label{fig:ablation_pca_random}
\end{figure}

\subsubsection{Directional Forcing and Defect Induction}
\label{sec:induction}

Next, we test whether artificially inducing curvature defects is sufficient to trigger early grokking.
Starting at step 500, we periodically inject additive weight updates aligned with the commutator direction (recomputed every 50 steps), with amplitudes $\alpha \in \{50, 100, 200, 500\}$ times the gradient step norm.
As a control, we apply kicks of equal magnitude along random orthogonal directions.

Across all tested amplitudes, neither commutator-aligned nor random kicks accelerate grokking relative to baseline (\Cref{fig:sustained_kicks}).
All 27/27 runs generalize at statistically indistinguishable times ($\sim$3200 steps, within seed-to-seed variability).
This negative result demonstrates that defect accumulation alone is insufficient to induce grokking, and that escape from the metastable regime requires coordinated motion along learned directions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figI8_sustained_kick_dose_response.png}
    \caption{Sustained directional kicks along the commutator (red) vs.\ random orthogonal (gray) directions, with kick magnitudes up to $500\times$ the gradient step norm applied every 50 steps.
    Left: mean grok step (3 seeds); right: grok success rate.
    Neither direction accelerates grokking beyond baseline variability (dashed line), confirming that the orthogonal defect is not sufficient to induce the phase transition.}
    \label{fig:sustained_kicks}
\end{figure}

\subsubsection{Replication Across Operations}
\label{sec:intervention_replication}

We repeat the projection experiments across all four grokking tasks: modular addition, subtraction, multiplication, and quadratic addition ($a^2 + b^2 \bmod 97$).
The dose--response relationship between projection strength and grokking delay is consistent across all operations (\Cref{fig:multiop_dose}), with complete suppression at full strength (0/12 seeds grok at $s = 1.0$).
At $s = 0.75$, grokking is delayed by 600--800 steps (20--25\% above baseline) across all four operations.
This universality suggests that the causal role of execution-manifold directions is not task-specific, but reflects a common geometric mechanism underlying algorithmic generalization.

\subsubsection{Summary of Interventions}
\label{sec:intervention_summary}

Taken together, these experiments establish a necessary--but--not--sufficient relationship between orthogonal gradient flow and grokking.
Constraining motion along the execution manifold prevents generalization with a smooth dose--response curve that is specific to the PCA directions and replicates across operations;
artificially increasing defect through directional forcing has no effect.
This asymmetry is consistent with the commutator defect serving as a \emph{signature} of the curvature barrier between memorization and generalization solutions---a structured reorganization of the optimization trajectory---rather than a directly manipulable cause of the phase transition.
Together, these interventions rule out purely correlational explanations of our earlier findings and establish a directional causal relationship between execution-manifold geometry and generalization.


% ══════════════════════════════════════════════════════════════════════════
\section{Discussion and Theoretical Connections}
\label{sec:discussion}
% ══════════════════════════════════════════════════════════════════════════

In this work, we investigated grokking through the lens of optimization geometry, focusing on curvature accumulation, trajectory--curvature alignment, and the emergence of low-dimensional execution manifolds.
Our results suggest that grokking corresponds to a dynamical transition in parameter space, characterized by escape from a metastable, high-curvature regime into a flat, structured solution manifold.
This perspective provides a unifying framework connecting grokking to several broader themes in learning theory and neural network optimization.

\subsection{Grokking as Metastable Escape in Curved Landscapes}
\label{sec:disc_metastable}

\emph{Thesis: grokking is best understood as escape from a metastable regime, not merely delayed learning.}

Across tasks and hyperparameter regimes, we observe that memorization confines training trajectories to regions of high curvature anisotropy, with commutator defects gradually increasing until a critical level triggers the generalization transition.
This is reminiscent of metastable escape in stochastic dynamical systems, where the defect magnitude functions as accumulated geometric tension and the learning rate controls damping.
Lower learning rates produce overdamped trajectories requiring substantial defect buildup ($\sim$30k steps at $\eta = 10^{-4}$), while higher learning rates facilitate rapid transitions ($\sim$1k steps at $\eta = 10^{-2}$; \Cref{sec:lr_sweep}).

\subsection{A Dynamical-Systems Interpretation}
\label{sec:disc_dynamical}

\emph{Thesis: grokking follows the classical dynamical-systems pattern of slow-manifold formation, transverse instability, and escape.}

Our findings admit a natural interpretation in the language of dynamical systems, proceeding through four phases:
\begin{enumerate}
    \item \textbf{Compression.} Early training collapses the weight trajectory onto a low-dimensional slow manifold (PC1 captures 70--94\% of variance; \Cref{sec:rank1}). The optimizer finds a rank-1 subspace and confines subsequent motion to it.
    \item \textbf{Transverse instability.} Curvature accumulates in the normal bundle of this manifold ($10$--$1000\times$ defect growth; \Cref{sec:curvature}), while the trajectory remains invariant ($\rho \approx 1.000$; \Cref{sec:invariant}). The system becomes increasingly sensitive to off-manifold perturbations.
    \item \textbf{Critical transition.} Weight decay provides a sustained pressure toward lower-norm solutions. Combined with the accumulated transverse curvature, this drives the trajectory through a bifurcation into the generalizing basin.
    \item \textbf{Reorganization.} The trajectory settles into a new regime characterized by lower curvature anisotropy along the optimization path---the post-grokking solution.
\end{enumerate}

This parallels the classical scenario of slow-manifold collapse followed by transverse bifurcation in dissipative dynamical systems.
The learning rate controls the damping: low $\eta$ produces overdamped dynamics with gradual defect buildup, high $\eta$ produces underdamped dynamics with rapid transitions (\Cref{sec:lr_sweep}).

\paragraph{Limitations of the dynamical-systems analogy.}
While this interpretation is supported by the temporal ordering (curvature precedes generalization) and the causal necessity of orthogonal gradient flow (\Cref{sec:interventions}), the available geometric signals---commutator defect, trajectory--curvature alignment, and invariance measure---do not combine into a \emph{predictive} composite diagnostic that discriminates grokking from non-grokking operations in advance.
The invariance measure is identically~1 across all conditions; trajectory--curvature alignment is noisy and overlapping; and defect magnitude only separates grokking from non-grokking operations retrospectively.
Moreover, the same grokking operations trained \emph{without} weight decay exhibit comparable defect acceleration without generalizing, confirming that curvature growth alone is insufficient---regularization pressure is the additional ingredient.
This suggests a fundamental limitation: the grokking transition may be \emph{locally unpredictable} from curvature measurements alone.
Local geometric diagnostics can detect that the system is approaching a bifurcation---the loss landscape is reorganizing, transverse instability is growing---but whether the bifurcation leads to generalization depends on a \emph{global} property of the landscape (the existence of a low-norm generalizing basin) interacting with \emph{exogenous} pressure (weight decay).
No purely local measurement can determine whether such a basin exists on the other side of the curvature barrier.
A complete dynamical-systems theory of grokking would therefore need to characterize not only the local curvature accumulation but also the global topology of the loss landscape that determines which transitions lead to generalization.

\subsection{Implicit Regularization and Low-Dimensional Structure}
\label{sec:disc_regularization}

\emph{Thesis: implicit regularization in grokking operates through the emergence of task-specific geometric structure, not merely norm or margin control.}

Following grokking, trajectories collapse onto low-dimensional execution manifolds (PC1 explains 70--94\% of variance; \Cref{sec:rank1}), accompanied by reduced curvature anisotropy along the trajectory.
The intervention experiments (\Cref{sec:interventions}) demonstrate that motion along these specific learned directions is necessary for grokking, while generic low-dimensional constraints are insufficient.
This suggests that implicit regularization in this setting operates through the progressive emergence of geometrically privileged subspaces.

\subsection{Scaling Behavior and Phase Diagrams}
\label{sec:disc_scaling}

\emph{Thesis: grokking exhibits a phase diagram with power-law scaling, paralleling phenomena in larger-scale systems.}

Our learning-rate sweeps reveal distinct overdamped, critically damped, and underdamped regimes (\Cref{sec:lr_sweep}), with grok time scaling approximately as $t_\text{grok} \propto \eta^{-1}$ and the curvature defect serving as an order parameter (sign test $p = 2^{-12}$; \Cref{sec:prediction}).
While our experiments operate in a small-model regime ($\sim$290k parameters), these scaling relationships parallel phenomena reported in large-scale language models, suggesting that grokking may represent a microscopic instance of optimization-driven phase transitions.

\subsection{Robustness, Flat Minima, and Quantization}
\label{sec:disc_robustness}

Recent empirical work has demonstrated that trained neural networks can tolerate substantial parameter quantization and compression with limited performance degradation.
Our results provide a geometric perspective on this robustness in the grokking regime.

Post-grokking solutions are characterized by reduced curvature anisotropy along the optimization trajectory: the commutator defect, while still nonzero, is concentrated orthogonally to the directions the optimizer traverses, and the invariance measure $\rho \approx 1.000$ (within numerical precision) indicates that curvature is confined to the normal bundle of the execution manifold.
Such regions, where curvature is confined to directions not visited by the optimizer, naturally support greater robustness to perturbations along the learned subspace.
In contrast, pre-grokking solutions reside in highly anisotropic, high-defect regions and are correspondingly more sensitive to perturbation.

This suggests that robustness to compression may be partly understood as a consequence of geometric reorganization during training, rather than solely as a byproduct of architectural or regularization choices.

\subsection{Connection to Mechanistic Interpretability}
\label{sec:disc_interpretability}

\emph{Thesis: the geometric transition during grokking corresponds to the stabilization of interpretable circuits.}

The formation of execution manifolds corresponds to the concentration of computation into low-dimensional subspaces, consistent with the emergence of interpretable circuits documented in prior work.
The collapse of curvature anisotropy during grokking indicates that these circuits become geometrically stabilized.
From this viewpoint, grokking marks the transition from distributed representations to structured, circuit-like organization---providing a potential bridge between optimization geometry and mechanistic interpretability.

\subsection{Limitations and Open Problems}
\label{sec:disc_limitations}

Our experiments are limited to relatively small Transformer models (2--3 layers, $\sim$290k parameters) and synthetic algorithmic tasks (modular arithmetic mod~97).
While these settings permit fine-grained geometric analysis, it remains unclear to what extent the observed phenomena---rank-1 manifolds, invariant execution submanifolds, predictive defect onset---generalize to large-scale language models and real-world datasets.

In addition, several of our diagnostic measures, including commutator defects (4 forward-backward passes per sample) and trajectory--curvature alignment, are computationally expensive and difficult to scale.
Developing efficient approximations and proxies for these geometric diagnostics remains an important direction for future work.

Finally, a complete theoretical characterization of the observed phase transitions remains open.
Deriving analytical models that capture defect accumulation, manifold formation, and damping-controlled dynamics represents a promising avenue for future research.

\subsection{Outlook}
\label{sec:disc_outlook}

Taken together, our results suggest that grokking reflects a geometric reorganization of the optimization landscape, governed by curvature, damping, and emergent low-dimensional structure.
By integrating dynamical, geometric, and causal analyses, this work provides a foundation for understanding delayed generalization as a phase transition in learning dynamics.

The commutator defect serves as a diagnostic for ongoing loss-landscape reorganization: monitoring it during training can detect geometric changes hundreds to tens of thousands of steps before they manifest in accuracy metrics.
However, defect growth alone does not discriminate which runs will ultimately generalize---non-grokking operations and unregularized training both exhibit curvature growth without generalization.
The diagnostic value lies in detecting \emph{that} the landscape is reorganizing, not in predicting \emph{whether} grokking will occur.
Developing composite diagnostics that combine curvature dynamics with regularization-sensitive signals remains an open direction.

We hope that this perspective will inform future studies of optimization, scaling, robustness, and interpretability in neural networks.


% ══════════════════════════════════════════════════════════════════════════
\section{Related Work}
\label{sec:related}
% ══════════════════════════════════════════════════════════════════════════

\paragraph{Grokking.}
\citet{power2022grokking} first observed delayed generalization in modular arithmetic.
\citet{nanda2023grokking} identified ``grokking circuits'' (Fourier-basis representations) in 1-layer models.
\citet{liu2022omnigrok} showed that grokking occurs broadly when weight decay or weight norm is controlled.
\citet{zhong2024clock} described clock and pizza representations in modular addition.
\citet{thilak2022slingshot} connected grokking to slingshot dynamics in adaptive optimizers.
\citet{lyu2024dichotomy} characterized the role of weight decay in separating memorization from generalization phases.
More recently, \citet{merrill2023tale} framed grokking as competition between sparse and dense subnetworks, \citet{varma2023explaining} explained it through circuit efficiency, \citet{davies2023unifying} connected grokking to double descent, and \citet{kumar2024grokking} characterized the lazy-to-rich training transition.
Our work complements these representational and optimization-theoretic perspectives with a geometric and causal analysis.

\paragraph{Loss landscape geometry and scaling.}
The study of loss landscape geometry in neural networks has a rich history \citep{li2018visualizing, draxler2018essentially, garipov2018loss}.
\citet{fort2019large} studied the curvature of the loss landscape during training.
Our commutator defect is related to the Lie bracket of gradient vector fields and measures non-commutativity of the optimization flow; similar ideas appear in the study of natural gradient methods \citep{amari1998natural} and the Fisher information geometry of neural networks.
The power-law scaling we observe in grok time vs.\ learning rate resonates with the broader scaling laws literature \citep{kaplan2020scaling}, though our analysis operates at a much smaller scale.

\paragraph{Intrinsic dimensionality.}
\citet{li2018measuring} showed that neural network optimization occurs in a low-dimensional subspace.
\citet{xu2026lowdim} demonstrated that attention weight trajectories during grokking in modular arithmetic lie on a low-dimensional execution manifold, with PC1 capturing the majority of variance.
A corrected version of that work includes random-subspace baseline controls showing that the execution basis captures $2$--$10\times$ more commutator energy than a random subspace of equal dimension.
The present work extends those findings by establishing that the execution manifold is invariant under the optimization dynamics (curvature confined to the normal bundle), adding analogous random baseline controls, demonstrating that curvature dynamics predict the generalization transition, and testing the causal role of orthogonal gradient flow through intervention experiments.


% ══════════════════════════════════════════════════════════════════════════
\section{Conclusion}
\label{sec:conclusion}
% ══════════════════════════════════════════════════════════════════════════

We have shown that the weight-space trajectory during grokking lies on a rank-1 invariant submanifold of parameter space---the execution manifold---and that loss-landscape curvature is confined to the normal bundle of this submanifold.
Curvature growth in the normal bundle consistently precedes generalization by 600--1600 steps, establishing a robust temporal ordering; however, non-grokking operations also exhibit moderate curvature growth without generalizing, so onset is a necessary precondition rather than a sufficient predictor.
These findings hold across six modular arithmetic operations, three random seeds, two weight-decay settings, a $100\times$ learning rate sweep, and two qualitatively different hyperparameter regimes ($200\times$ range in training timescale).
Causal intervention experiments close the loop: suppressing orthogonal gradient flow prevents grokking with a monotonic dose--response across four operations, while artificially boosting curvature defects has no effect, establishing that normal-bundle curvature growth is mechanistically necessary for the generalization transition.
The geometric picture---invariant execution manifold, orthogonal curvature confinement, necessary-but-not-sufficient temporal ordering, and causal confirmation via interventions---provides a new lens for understanding the grokking phenomenon and suggests that monitoring gradient non-commutativity during training may serve as a diagnostic for ongoing loss-landscape reorganization, even when generalization is not yet observable in accuracy metrics.

\paragraph{Reproducibility.}
All code and figures are available at \url{https://github.com/skydancerosel/grokking-integrability}.
Total compute for full reproduction is approximately 9 hours on a single Apple M-series machine.


% ══════════════════════════════════════════════════════════════════════════
% References
% ══════════════════════════════════════════════════════════════════════════

\bibliographystyle{plainnat}
\bibliography{references}

% ══════════════════════════════════════════════════════════════════════════
\appendix
\section{Additional Figures}
\label{app:figures}
% ══════════════════════════════════════════════════════════════════════════

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figB_pc1_heatmap.png}
    \caption{PC1\% heatmap by operation and weight matrix (last layer, wd=1.0). All weight matrices show high PC1\% for grokking operations.}
    \label{fig:heatmap}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figG_per_weight_crossop.png}
    \caption{Per-weight-matrix PC1\% comparison across operations (grok vs.\ no-wd).}
    \label{fig:per_weight}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figH_regime_comparison.png}
        \caption{Slow vs.\ fast regime PC1\%. The slow regime shows lower PC1\%, but still well above the null model.}
        \label{fig:regime_pca}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figI_pc1_drop_decomposition.png}
        \caption{Decomposition of PC1\% drop between regimes: which hyperparameter drives the difference.}
        \label{fig:decomposition}
    \end{subfigure}
    \caption{Regime comparison for PCA concentration.}
    \label{fig:regime_pca_detail}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figM_defect_integrability.png}
        \caption{Combined view: defect magnitude and invariance measure over training.}
        \label{fig:combined}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figN_attn_weight_fraction.png}
        \caption{Attention weight fraction of commutator defect.}
        \label{fig:attn_fraction}
    \end{subfigure}
    \caption{Commutator analysis details.}
    \label{fig:comm_details}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figO_trajectory_alignment.png}
        \caption{Trajectory-curvature alignment over training.}
        \label{fig:alignment}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figQ_alignment_ratio.png}
        \caption{Alignment ratio vs.\ random baseline: the trajectory does not prefer curvature directions.}
        \label{fig:alignment_ratio}
    \end{subfigure}
    \caption{Converse analysis: the weight trajectory avoids high-curvature directions.}
    \label{fig:converse}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figV_temporal_add_seeds.png}
    \caption{Temporal traces for $(a+b) \bmod 97$ with 3 seed overlays, showing consistency of the invariance and defect patterns across seeds.}
    \label{fig:temporal_seeds}
\end{figure}

\end{document}
