@inproceedings{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  booktitle={ICLR 2022 Workshop on MATH-AI},
  year={2022},
  url={https://arxiv.org/abs/2201.02177}
}

@article{nanda2023grokking,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas and Michaud, Eric J and Tegmark, Max and Williams, Mike},
  journal={arXiv preprint arXiv:2210.01117},
  year={2022}
}

@article{zhong2024clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{thilak2022slingshot,
  title={The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon},
  author={Thilak, Vimal and Littwin, Etai and Zhai, Shuangfei and Saremi, Omid and Paiss, Roni and Susskind, Joshua},
  journal={arXiv preprint arXiv:2206.04817},
  year={2022}
}

@article{lyu2024dichotomy,
  title={Dichotomy of early and late phase implicit biases can provably induce grokking},
  author={Lyu, Kaifeng and Jin, Jikai and Li, Zhiyuan and Du, Simon S and Lee, Jason D and Hu, Wei},
  journal={arXiv preprint arXiv:2311.18817},
  year={2024}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A},
  booktitle={International Conference on Machine Learning},
  pages={1309--1318},
  year={2018}
}

@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of {DNN}s},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{fort2019large,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-ichi},
  journal={Neural Computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998}
}

@inproceedings{li2018measuring,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{xu2026lowdim,
  title={Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks},
  author={Xu, Yongzhong},
  journal={arXiv preprint arXiv:2602.10496},
  year={2026},
  url={https://arxiv.org/abs/2602.10496}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{merrill2023tale,
  title={A tale of two circuits: Grokking as competition of sparse and dense subnetworks},
  author={Merrill, William and Tsilivis, Nikolaos and Shukla, Aman},
  journal={arXiv preprint arXiv:2303.11873},
  year={2023}
}

@article{varma2023explaining,
  title={Explaining grokking through circuit efficiency},
  author={Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Nanda, Neel},
  journal={arXiv preprint arXiv:2309.02390},
  year={2023}
}

@article{davies2023unifying,
  title={Unifying grokking and double descent},
  author={Davies, Xander and Langosco, Lauro and Krueger, David},
  journal={arXiv preprint arXiv:2303.06173},
  year={2023}
}

@article{kumar2024grokking,
  title={Grokking as the transition from lazy to rich training dynamics},
  author={Kumar, Tanishq and Bordelon, Blake and Gershman, Samuel J and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2310.06110},
  year={2024}
}
